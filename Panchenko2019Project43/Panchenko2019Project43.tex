\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{NotationStyle}
\usepackage{ dsfont }
\usepackage{comment}
%\NOREVIEWERNOTES
\title
    [Получение простой выборки на выходе слоя нейронной сети] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Получение простой выборки на выходе слоя нейронной сети}
\author
    [Панченко~С.\,К.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Панченко~С.\,К., Гадаев~Т.\,Т., Грабовой~А.\,В., Стрижов~В.\,В.} % основной список авторов, выводимый в оглавление
    [Панченко~С.\,К.$^1$, Гадаев~Т.\,Т.$^1$, Грабовой~А.\,В.$^1$, Стрижов~В.\,В.] %список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Грабовой~А.\,В.
    Консультант:  Гадаев~Т.\,Т.}
\email
    {panchenko.sk@phystech.edu}
\organization
    {$^1$Московский Физико-Технический Институт}%; $^2$Организация}
\abstract
    {Границы применимости многочисленных методов математической статистики, применяемых в анализе, обуславливаются знанием вероятностной природы данных, которая, как правило, заранее неизвестна. В данной статье предлагается объединение целого комплекса статистических критериев в универсальный инструмент исследования распределений, порождающих выборки. Также предлагается алгоритм усовершенствования нейронной сети на основе анализа выборки, полученной на выходе её предпоследнего слоя, с помощью предложенного инструмента.

\bigskip
\textbf{Ключевые слова}: \emph {тестирование гипотез, полносвязная нейросеть,
условия Гаусса-Маркова}.}

\begin{comment}
\titleEng
    {JMLDA paper example: file jmlda-example.tex}
\authorEng
    {Author~F.\,S.$^1$, CoAuthor~F.\,S.$^2$, Name~F.\,S.$^2$}
\organizationEng
    {$^1$Organization; $^2$Organization}
\abstractEng
    {This document is an example of paper prepared with \LaTeXe\
    typesetting system and style file \texttt{jmlda.sty}.

    \bigskip
    \textbf{Keywords}: \emph{keyword, keyword, more keywords}.}
\end{comment}

\begin{document}
\maketitle
%\linenumbers
\section{Введение}
Знание того, какое распределение породило признаковое описание выборки или шум в заданных ответах, позволяет обоснованно применять к построенной модели разнообразные критерии математической статистики - мощный аппарат, позволяющий анализировать вероятностные закономерности в данных. Подобный анализ не только помогает интерпретировать модель и улучшать её предсказательные способности, но также позволяет производить отбор признаков по значимости, строить доверительные интервалы, и многое другое. Выборку, для которой известно семейство и параметры распределений, породивших признаковое описание её объектов и, возможно, шум в заданных ответах, назовём простой. На практике, однако, эти распределения часто заранее неизвестны, и изучать их вероятностный характер при необходимости приходится самостоятельно. Именно построение универсального алгоритма, позволяющего установить вероятностный закон, породивший данные, или, другими словами, алгоритма, исследующего выборку на простоту, и становится целью исследования. \par
Одно из ключевых применений искомого алгоритма находит своё место в совершенствовании нейронных сетей. Как известно, часто результат работы нейронной сети можно рассмотреть как применение некоторой обобщенно-линейной модели к выходам предпоследнего слоя. Совокупность слоев нейронной сети, вплоть до предпоследнего, можно в такой интерпретации рассматривать как композицию в общем случае нелинейных преобразований признакового описания исходной выборки. В итоге на предпоследнем слое сети формируется преобразованная выборка, которая исследуется на простоту с помощью построенного инструмента. Результаты этого исследования помогут усовершенствовать имеющуюся сеть, к примеру, с помощью отбора признаков по значимости окажется возможным предложить технику прореживания сети без потери качества, что является актуальной задачей глубокого обучения. \par
Как уже упоминалось выше, проводимое исследование существенно опирается на теорию и приложения математической статистики. В первую очередь рассматриваются такие статистические критерии, как критерий Вальда, тест Уайта, тесты Голдфелда-Кванта и Дарби-Ватсона, тесты хи-квадрат, Жарка-Бера и Шапиро-Уилка. На основе этих критериев в применении к различным выборкам и вырабатывается искомый алгоритм. Вычислительный эксперимент проводится на наборе стандартных выборок, а также на синтетических и реальных данных. \par
В качестве основных источников исследованию послужат такие фундаментальные публикации, как \cite{books/lib/Bishop07}, ..., а также следующие статьи: ... .

\section{Постановка задачи}
Пусть $\bX \in \mathds{R}^{m \times n}$ - признаковое описание заданной обучающей выборки с вектором ответов $\mathbf{y} \in \mathds{R}^m$. Пусть также имеется исходная модель - полносвязная нейронная сеть-перцептрон $$f(\mathbf{x}) = f(\mathbf{x}|\mathbf{w}):\mathds{R}^n \rightarrow \mathds{R},$$ где $\mathbf{w}$ - совокупность параметров нейронной сети. \par
В самом общем виде нейронную сеть можно представить в виде последовательности преобразований вида $$f_i(\mathbf{z}):\mathds{R}^{k_{i-1}} \rightarrow \mathds{R}^{k_i}, f_i(\mathbf{z}) = \mathbf{r}_i(\mathbf{W}_i\times\mathbf{z} + \mathbf{b}_i)$$ - преобразования во внутренних слоях сети, где $i = 1, ..., s-1$ ($s$ - количество слоев нейронной сети), $\mathbf{W}_i \in \mathds{R}^{k_i \times k_{i-1}}$ - матрица весов $i$-ого слоя, $\mathbf{b}_i \in \mathds{R}^{k_i}$ - вектор порогов активации $i$-ого слоя, $\mathbf{r}_i: \mathds{R}^{k_i} \rightarrow \mathds{R}^{k_i}$ - функция активации $i$-ого слоя ($k_0 = n$). \par Завершает цепочку преобразований функция активации последнего слоя, представляющая собой (в простейшем случае) линейную модель над выходами предпоследнего слоя:
$$f_s(\mathbf{z}):\mathds{R}^{k_{s-1}} \rightarrow \mathds{R}, f_s(\mathbf{z}) = \langle\mathbf{w}_s,\mathbf{z}\rangle + b_s,$$ где  $\mathbf{w}_s \in \mathds{R}^k_{s-1}$ - вектор весов, а $b_s \in \mathds{R}$ - свободный член линейной модели последнего слоя. \par
Итого, результат действия нейронной сети на объект представляется в виде композиции:$$f(\mathbf{x}) = (f_s \circ f_{s-1} \circ ... \circ f_2 \circ f_1)(\mathbf{x}) = f_s((f_{s-1} \circ ... \circ f_2 \circ f_1)(\mathbf{x})) = g(h(\mathbf{x})) = (g \circ h)(\mathbf{x}),$$
где $g = f_s, g(\mathbf{x}):\mathds{R}^{l} \rightarrow \mathds{R}$ (здесь $l := k_{s-1}$), $h = f_{s-1} \circ ... \circ f_2 \circ f_1, h(\mathbf{x}):\mathds{R}^n \rightarrow \mathds{R}^l.$ 
\newline
\par
Рассмотрим теперь $\mathbf{Z}=[h(\mathbf{x}_1)^{\mathsf{T}},...,h(\mathbf{x}_m)^{\mathsf{T}}]^{\mathsf{T}} \in \mathds{R}^{m \times l}$ - совокупность преобразованных внутренними слоями сети исходных векторов признаков, или, другими словами, преобразованное сетью признаковое описание заднной обучающей выборки. Результат действия исходной модели $f(\mathbf{x})$ на объект $\mathbf{x}_i \in \mathbf{X}$ эквивалентен результату действия последнего слоя сети, т.е. линейной модели $g(\mathbf{z}) =  \langle\mathbf{w}_s,\mathbf{z}\rangle + b_s,$ на преобразованный объект $\mathbf{z}_i = h(\mathbf{x}_i) \in \mathbf{Z}$.\par
Итак, обратимся теперь к исследованию вышеупомянутой модели $g(\mathbf{z})$ применительно к выборке $\mathfrak{D} = \{(\mathbf{z}_i, y_i)|$ $ \mathbf{z}_i = h(\mathbf{x}_i), \mathbf{x}_i \in X, i = 1,...,m\}$. В первую очередь имеющаяся модель ислледуется на удовлетворение условиям Гаусса-Маркова. Сформулируем их в виде следующей теоремы: \par
\begin{Theorem}
Рассматривается модель регрессии, в которой наблюдения $\mathbf{Y}$ связаны с $\mathbf{X}$ следующей зависимостью: $\mathbf{Y}_i = \beta_1 + \langle\beta_2, \mathbf{X}_i\rangle + \varepsilon_i$. На основе $n$ выборочных наблюдений оценивается уравнение регрессии $\hat{\mathbf{Y}_i} = \hat\beta_1 + \langle\hat\beta_2, \mathbf{X}_i\rangle$. Теорема Гаусса—Маркова гласит:

Если данные обладают следующими свойствами:
\begin{itemize}
\item Модель данных правильно специфицирована; 
\item Все $\mathbf{X}_{i}$ детерминированы и не все равны между собой;
\item Ошибки не носят систематического характера, то есть $\mathbb{E}(\varepsilon_i) = 0\ \forall i$; 
\item Дисперсия ошибок одинакова и равна некоторой $\sigma ^{2}$;
\item Ошибки некоррелированы, то есть $\mathop{\mathrm{Cov}}(\varepsilon_i,\varepsilon_j)=0\ \forall i,j;$ 
\end{itemize}
— то в этих условиях оценки метода наименьших квадратов оптимальны в классе линейных несмещённых оценок.
\end{Theorem}
\par
В случае, когда модель $g(\mathbf{z})$ c выборкой $\mathbf{Z}$ не удовлятворяет условиям Гаусса-Маркова, предлагается усовершенствовать исходную сеть, модифицировав параметры её внутренних слоев и получив, таким образом, новую функцию $h'(\mathbf{x})$, такую, что преобразованная выборка $h'(\mathbf{X})$ была простой, а линейная модель уже удовлетворяла бы условиям Гаусса-Маркова. Эта функция также должна быть выбрана из следующих соображений: имеющаяся новая модель - модифицированная нейронная сеть $f'(\mathbf{x}) = g(h'(\mathbf{x}))$ должна иметь меньшую среднеквадратичную ошибку на тестовой выборке (предполагая разделение исходной выборки на обучение и тест). Другими словами, ищется такая модификация $h'$, что $$Q(\mathbf{X}|f') \leq Q(\mathbf{X}|f),$$ где $Q(\mathbf{X}|f) = \frac{1}{m}\sum_{j=1}^m(f(\mathbf{x}_m) - \mathbf{y}_m)^2$ - среднеквадратичкая ошибка алгоритма на выборке. Поиску такой функции $h'$ будет посвящен следующий раздел.

\begin{comment}
\section{Название раздела}
Данный документ демонстрирует оформление статьи,
подаваемой в электронную систему подачи статей \url{http://jmlda.org/papers} для публикации в журнале <<Машинной обучение и анализ данных>>.
Более подробные инструкции по~стилевому файлу \texttt{jmlda.sty}
и~использованию издательской системы \LaTeXe\
находятся в~документе \texttt{authors-guide.pdf}.
Работу над статьёй удобно начинать с~правки \TeX-файла данного документа.

\paragraph{Название параграфа.}
%Первый раздел может содержать формальную постановку задачи,
%основные определения и~обозначения,
%известные факты, необходимые для понимания основных результатов работы,
%и~т.\,п.
Нет ограничений на~количество разделов и~параграфов в~статье.
Разделы и~параграфы не~нумеруются.

\paragraph{Теоретическую часть работы} желательно структурировать
с~помощью окружений
Def, Axiom, Hypothesis, Problem, Lemma, Theorem, Corollary, State, Example, Remark.

\begin{Def}
    Математический текст \emph{хорошо структурирован},
    если в~нём выделены определения, теоремы, утверждения, примеры, и~т.\,д.,
    а~неформальные рассуждения (мотивации, интерпретации)
    вынесены в~отдельные параграфы.
\end{Def}

\begin{State}
    Мотивации и~интерпретации наиболее важны для понимания сути работы.
\end{State}

\begin{Theorem}
    Не~менее $90\%$ коллег, заинтересовавшихся Вашей статьёй,
    прочитают в~ней не~более~$10\%$ текста.
\end{Theorem}

\begin{Proof}
    Причём это будут именно те~разделы, которые не содержат формул.
\end{Proof}

\begin{Remark}
    Выше показано применение окружений
    Def, Theorem, State, Remark, Proof.
\end{Remark}

\section{Некоторые формулы}

Образец формулы: $f(x_i,\alpha^\gamma)$.

Образец выключной формулы без номера:
\[
    y(x,\alpha) =
    \begin{cases}
        -1, & \text{если } f(x,\alpha)<0;  \\
        +1, & \text{если } f(x,\alpha)\geq 0.
    \end{cases}
\]

Образец выключной формулы с номером:
\begin{equation}
\label{eq:cases}
    y(x,\alpha) =
    \begin{cases}
        -1, & \text{если } f(x,\alpha)<0;  \\
        +1, & \text{если } f(x,\alpha)\geq 0.
    \end{cases}
\end{equation}

Образец выключной формулы, разбитой на две строки с~помощью окружения align:
\begin{align}
    R'_N(F)
        = \frac1N \sum_{i=1}^N
        \Bigl(
            & P(+1\cond x_i) C\bigl(+1,F(x_i)\bigr)+{}
        \notag % подавили номер у первой строки
    \\ {}+{}
            & P(-1\cond x_i) C\bigl(-1,F(x_i)\bigr)
        \Bigr).
        \label{eq:R(F)}
\end{align}

Образцы ссылок: формулы~\eqref{eq:cases} и~\eqref{eq:R(F)}.

\section{Пример илюстрации}

Рисунки вставляются командой \verb|\includegraphics|,
желательно с~выравниванием по~ширине колонки: \verb|[width=\linewidth]|.

Практически все популярные пакеты рисуют графики с подписями, которые трудно читать на бумаге и на слайдах из-за малого размера шрифта. Шрифт на графиках (подписи осей и цифры на осях) должны быть такого же размера, что и основной текст.

\begin{figure}[h]
  \subfloat[Первый рисунок]{\includegraphics[width=0.5\textwidth]{figExample1}}
  \subfloat[Второй рисунок]{\includegraphics[width=0.5\textwidth]{figExample2}}\\
\caption{Подпись должна размещаться под рисунком. }
\label{fg:Example}
\end{figure}

При значительном количестве рисунков рекомендуется группировать иx в одном окружении \verb|{figure}|, как это сделано на рис.~\ref{fg:Example}.

\section{Пример таблицы}
Подпись делается \emph{над таблицей}, см.~таблицу~\ref{TabExample}.


\begin{table}[t]%\small
    \caption{Подпись размещается над таблицей.}
    \label{TabExample}
    \centering\medskip%\tabcolsep=2pt%\small
    \begin{tabular}{lrrr}
    \headline
        Задача
            & \multicolumn{1}{c}{CCEL}
            & \multicolumn{1}{c}{boosting} \\
    \headline
        {\tt Cancer}
            & $\mathbf{3.46}  \pm 0.37$ (3.16)
            & $4.14 \pm 1.48$ \\
        {\tt German}
            & $\mathbf{25.78} \pm 0.65$ (1.74)
            & $29.48 \pm 0.93$ \\
        {\tt Hepatitis}
            & $18.38 \pm 1.43$ (2.87)
            & $19.90 \pm 1.80$ \\
    \hline
    \end{tabular}
\end{table}

\section{Заключение}
Желательно, чтобы этот раздел был, причём он не~должен дословно повторять аннотацию.
Обычно здесь отмечают,
каких результатов удалось добиться,
какие проблемы остались открытыми.
\end{comment}

\bibliographystyle{plain}
\bibliography{Panchenko2019Project43}
%\begin{thebibliography}{1}

%\bibitem{author09anyscience}
%    \BibAuthor{Author\;N.}
%    \BibTitle{Paper title}~//
%    \BibJournal{10-th Int'l. Conf. on Anyscience}, 2009.  Vol.\,11, No.\,1.  Pp.\,111--122.
%\bibitem{myHandbook}
%    \BibAuthor{Автор\;И.\,О.}
%    Название книги.
%    Город: Издательство, 2009. 314~с.
%\bibitem{author09first-word-of-the-title}
%    \BibAuthor{Автор\;И.\,О.}
%    \BibTitle{Название статьи}~//
%    \BibJournal{Название конференции или сборника},
%    Город:~Изд-во, 2009.  С.\,5--6.
%\bibitem{author-and-co2007}
%    \BibAuthor{Автор\;И.\,О., Соавтор\;И.\,О.}
%    \BibTitle{Название статьи}~//
%    \BibJournal{Название журнала}. 2007. Т.\,38, \No\,5. С.\,54--62.
%\bibitem{bibUsefulUrl}
%    \BibUrl{www.site.ru}~---
%    Название сайта.  2007.
%\bibitem{voron06latex}
%    \BibAuthor{Воронцов~К.\,В.}
%    \LaTeXe\ в~примерах.
%    2006.
%    \BibUrl{http://www.ccas.ru/voron/latex.html}.
%\bibitem{Lvovsky03}
%    \BibAuthor{Львовский~С.\,М.} Набор и вёрстка в пакете~\LaTeX.
%    3-е издание.
%    Москва:~МЦHМО, 2003.  448~с.
%\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}